{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "                            (#%(//***,.                                         \n",
    "                          %@&&&&&&&&%&&&&&&#                                    \n",
    "                       ,@@@@@&&%#(/*,,*(#%&&@&. ,#%&&&&&@&&#,                   \n",
    "                      %&@@@@&&%(*,.....,*(#%&&@&&@@@@@&%&&@@@@@@@&              \n",
    "                     /%@&@@@&%%(/******/(##%&&@@&@@&#((////(((#%&@@@@           \n",
    "            /&@@@@@@&%@@@&@@&%%###(((#%%%&&&&&&&##%&#///*/(&@@@&%%&@@@#         \n",
    "        (@@@@&/@@@@@@@@#%@@@@&&%%%%*/#&%#%&&&@@@@@@&#//&%                       \n",
    "      @@@&&((##(%&@@@@@@&%@%@&@(%#%%%%%%,./(/.(/&((&@.                          \n",
    "    &@@%//***,****(#%&@@@@@@@@%&*/#/.*(///*,//.#%%,&                            \n",
    "   &@@(/**,,,,,,,,,,/&&@@@@@@@@&. ((/  /#%####%&&&(&                            \n",
    "  &@@#(#%@@@&*        %@@@@@@@@&*#%&/%((((%&&%%&&@@&                            \n",
    " .@@@@@                   (#&&&&@@@@&&&//,,,(/(##/                             ,\n",
    " .@&                      %@@&&(%(,*(/,..*(#/(%#((#%(.                       ,.@\n",
    "  .                   .(&%&@@@&%##**(#@/,  %@%%@#/#,%&&&                    ,.@@\n",
    "                  *%&@@&(/#%&@&@@@#(.. .,,*/*&@@@///,%%&@%                 ,.@@@\n",
    "                &@@@&&&%&@*,(#@@&@@@@@@(. ,&@&%@@&(,%,&&@@/              .,(@@@@\n",
    "               &@@%%(/#%&@@,,*(%@%@@@@@@@&##/(%&&%&#/#&@@@&           ..*.@@@@@@\n",
    "               @@@&%%%&&&@@@,%,&%&&&@@&%((&(%&@@@@%@#&&&&&@&.        ..*,@@@@@@@\n",
    "               #@@@&%(((%&&@%#,(%%@@&@&%*@@@@@@@@@@@%@&&/(#&&*...  ...*,@@@@@@@@\n",
    "               .%&&&(  ,(%&&@&*(&&&&&%/,&@&(@@&%&@@@&@@@&**#&&&,. ../*(@@@@@@@@&\n",
    "         .......%&@@@&&&@@@@@&/&&&%%#/(&@&(%%@%%%%@&@&&@@&&./#%%* ./*&@@@@@@@@@@\n",
    "##############(,*@@&&&&&&&@@@&&@@@@&#*#&&@&#%&%%#%%%@@&%@&&&#,(%&%*/@@@@@@@@@@@@\n",
    "##############(*.@@@%%#&&&@@@&@&%#(*#&@&#(#&&%&&##(,%&&&%%&@&&&@%/&@@@@@@@@@@@@@\n",
    "((##########(((/.%@@@#(#%&&@&&&%(*,(#%%&&(%#%#%&%#(,.#%&@@&@(,,%*#@@@@@@@@@@@@@@\n",
    "//(##########(,...@@@@@&&%@@&&%#((#%&&&%&%%&%%%&&%    ##%&@@@@/*&%@@@@@@@@@@@@@@\n",
    "(###########((/,,.(@@@&/,(@&&&&%%&&&&&%%&&&%%%%&&........%%%%**@@@@@@@@&@@@@@@@@\n",
    "###(,,,,*/(((((*,(%@@@@@@@@&&&&&&&@&%/&&%#%%%%&&#./#(*//(((%*/@@@@@@@@@@@@@@@@@@\n",
    ",*###(((((((((((#&&@@&@@@@&@@&&@@&&(&#**,**,,,********//((%*#@@@@@@@@@@@@@@@@@@@\n",
    "///////////////(#%&&@@&@@@@@@@@@&(**,*,,,,,,,,,*%//%##(##(*&@@@@@@@@@@@@@@@@@@@@\n",
    "///////////////////(##%&@@@@@@@&%******&/(##%###########/*@@@@@@@@@@@@@@@@@@@@@@\n",
    "////////////////////((#%%%%&@@@&&#,(/#%*,*/*.&%########//@@@@@@@@@@@@@@@@@@@@@@&\n",
    "%%(*/////////////////////(///////#@@@@@@@@/(%*,*/(.,&&*(@@@@@@@@@@@@@@@@@@@@@@(/\n",
    "```\n",
    "# Goblin Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json \n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from models.fnn import PoseFFNN, get_accuracy, train\n",
    "from helper import plot, get_pose_names, center_chest\n",
    "from dataloader.dataloader import load_data, PoseLoader\n",
    "from confusion_matrix import get_confusion_matrix, print_worst_cm\n",
    "\n",
    "os.environ['MEDIAPIPE_BACKEND'] = 'gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing mediapipe pose class.\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Setting up the Pose function.\n",
    "mediaPipe = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.3, model_complexity=2)\n",
    "\n",
    "# Initializing mediapipe drawing class, useful for annotation.\n",
    "mp_drawing = mp.solutions.drawing_utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the pose names\n",
    "poses = get_pose_names()\n",
    "\n",
    "# Load the pose data by type\n",
    "rawTrain, rawVal, rawTest = load_data(poses, type='raw')\n",
    "\n",
    "# Data is loaded in batches and labels are one-hot encoded\n",
    "train_data = PoseLoader(rawTrain, 'train', oneHot=True)\n",
    "val_data = PoseLoader(rawVal, 'val', oneHot=True)\n",
    "test_data = PoseLoader(rawTest, 'test', oneHot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_nuts():\n",
    "\n",
    "    best_model, best_val_acc = None, 0\n",
    "    best_lr, best_L2_reg, best_batch_size = None, None, None\n",
    "\n",
    "    # Define hyperparameter ranges for grid search\n",
    "    lr_range = [0.001, 0.003, 0.006]\n",
    "    L2_reg_range = [0.0001, 0.0003, 0.0006]\n",
    "    batch_size_range = [8, 16, 32, 64]\n",
    "    seed = 42\n",
    "    \n",
    "    all_val_accs = {}\n",
    "    for lr in lr_range:\n",
    "        all_val_accs[lr] = {}\n",
    "        for L2_reg in L2_reg_range:\n",
    "            all_val_accs[lr][L2_reg] = {}\n",
    "            for batch_size in batch_size_range:\n",
    "\n",
    "                print(f'lr : {lr} | l2 : {L2_reg} | bs : {batch_size}')\n",
    "\n",
    "                torch.manual_seed(seed)\n",
    "\n",
    "                # Define your model\n",
    "                model = PoseFFNN(input_dim=69, output_dim=82)\n",
    "\n",
    "                # Set up your optimizer\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=L2_reg)\n",
    "\n",
    "                # Set up your loss function\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                epochs = 75\n",
    "                train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "                model, this_val_acc = train(model, train_loader, val_data, optimizer, criterion, epochs, batch_size)\n",
    "\n",
    "                all_val_accs[lr][L2_reg][batch_size] = this_val_acc\n",
    "\n",
    "                if this_val_acc > best_val_acc:\n",
    "                    best_val_acc = this_val_acc\n",
    "                    best_model = copy.deepcopy(model)\n",
    "                    best_lr = lr\n",
    "                    best_L2_reg = L2_reg\n",
    "                    best_batch_size = batch_size\n",
    "\n",
    "    print('Best Hyperparameters')\n",
    "    print('learning rate :', best_lr)\n",
    "    print('weight decay :', best_L2_reg)\n",
    "    print('batch size :', best_batch_size)\n",
    "    print()\n",
    "\n",
    "    return best_model, all_val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, all_val_accs = go_nuts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"trackVal/midTanh_val_acc.json\", \"w\") as outfile:\n",
    "    json.dump(all_val_accs, outfile, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('val accuracy :', get_accuracy(model, val_data, device))\n",
    "print('test accuracy :', get_accuracy(model, test_data, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestAcc = 0\n",
    "bestLr, bestL2, bestBatch = 0, 0, 0\n",
    "\n",
    "for lr in all_val_accs:\n",
    "    for l2 in all_val_accs[lr]:\n",
    "        for batch_size in all_val_accs[lr][l2]:\n",
    "            if all_val_accs[lr][l2][batch_size] > bestAcc:\n",
    "                bestAcc = all_val_accs[lr][l2][batch_size]\n",
    "                bestLr = lr\n",
    "                bestL2 = l2\n",
    "                bestBatch = batch_size\n",
    "\n",
    "f = open('val_acc.json')\n",
    "old_val_accs = json.load(f)\n",
    "f.close()\n",
    "\n",
    "for lr in old_val_accs:\n",
    "    for l2 in old_val_accs[lr]:\n",
    "        for batch_size in old_val_accs[lr][l2]:\n",
    "            if old_val_accs[lr][l2][batch_size] > bestAcc:\n",
    "                bestAcc = old_val_accs[lr][l2][batch_size]\n",
    "                bestLr = lr\n",
    "                bestL2 = l2\n",
    "                bestBatch = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bestAcc)\n",
    "print(bestLr)\n",
    "print(bestL2)\n",
    "print(bestBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitons = []\n",
    "true_labels = []\n",
    "\n",
    "totalVals = len(test_data)\n",
    "for i in range(totalVals):\n",
    "    output = model(test_data[i]['input'].to(device))\n",
    "    predicitons.append(output.argmax(dim=0, keepdim=True))\n",
    "    true_labels.append(test_data[i]['label'].argmax(dim=0, keepdim=True).to(device))\n",
    "\n",
    "predicitons = [i.item() for i in predicitons]\n",
    "true_labels = [i.item() for i in true_labels]\n",
    "\n",
    "cm = get_confusion_matrix(true_labels, predicitons)\n",
    "print_worst_cm(cm, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model parameters to a file\n",
    "torch.save(model.state_dict(), 'models/tanh_parameters.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model parameters to a file\n",
    "torch.save(model.state_dict(), 'models/fnn_parameters.pth')\n",
    "\n",
    "# Define a new instance of the model\n",
    "model = PoseFFNN(input_dim=69, output_dim=82)\n",
    "\n",
    "# Load the saved model parameters into the new model\n",
    "model.load_state_dict(torch.load('models/fnn_parameters.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the video file\n",
    "video = cv2.VideoCapture('yoga.mp4')\n",
    "\n",
    "# initialize an empty list to store the images\n",
    "images = []\n",
    "\n",
    "# loop through the video frames\n",
    "while video.isOpened():\n",
    "    # read the current frame\n",
    "    ret, frame = video.read()\n",
    "    # check if the frame was read successfully\n",
    "    if ret:\n",
    "        # append the frame to the images list\n",
    "        images.append(frame)\n",
    "    else:\n",
    "        # break the loop if there are no more frames\n",
    "        break\n",
    "\n",
    "# release the video object\n",
    "video.release()\n",
    "\n",
    "# print the number of images extracted from the video\n",
    "print(f\"Total images extracted: {len(images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, image in enumerate(images):\n",
    "    if i % 6 == 0:\n",
    "        clear_output(wait=True)\n",
    "        imageRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = mediaPipe.process(imageRGB)\n",
    "        if results.pose_landmarks:\n",
    "            featureVector = center_chest(results.pose_landmarks.landmark)\n",
    "            plot(featureVector)\n",
    "            featureVector = torch.Tensor(featureVector.flatten())\n",
    "            pred = model(featureVector.to(device)).argmax(dim=0, keepdim=True)\n",
    "            print(poses[pred])\n",
    "        else:\n",
    "            print('bad image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the default camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Loop through each frame from the camera\n",
    "i = 0\n",
    "while True:\n",
    "\n",
    "    # Read the current frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # If the frame was read successfully\n",
    "    if ret:\n",
    "        i += 1\n",
    "        if i % 3 == 0:\n",
    "\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            imageRGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = mediaPipe.process(imageRGB)\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                featureVector = center_chest(results.pose_landmarks.landmark)\n",
    "                plot(featureVector)\n",
    "                featureVector = torch.Tensor(featureVector.flatten())\n",
    "                pred = model(featureVector.to(device)).argmax(dim=0, keepdim=True)\n",
    "                print(poses[pred])\n",
    "            else:\n",
    "                print('bad image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forMedia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca103a73504f546ece912beb0e22110d06c2f833e99fc57d6bf4b25d9b1ae21c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
